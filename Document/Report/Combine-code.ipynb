{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan 22 18:47:04 2019\n",
    "\n",
    "@author: Admin\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#import timeit\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import namedtuple\n",
    "from requests_futures.sessions import FuturesSession\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Admin\\Desktop\\chromedriver.exe')\n",
    "# listkey để chứa từ khóa tìm kiếm\n",
    "listkey=['abc','bcd','def']\n",
    "# tuy thuoc vao tung trang web, từng yêu cầu, mà số lượng thành phần của nametuple tăng hoặc giảm cho hợp lý\n",
    "CourseDetailed = namedtuple('CourseDetailed', 'title, company, description')\n",
    "start= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm details để lấy thông tin từ từng trang web\n",
    "def detail(response):\n",
    "    # lần lượt tìm kiếm và lấy thông tin từ từng div class tương ứng\n",
    "    soup = bs(response.text, 'lxml')\n",
    "    tit = soup.find('div', attrs={'class':'content bt3-col-xs-12 bt3-col-sm-12 bt3-col-md-12 bt3-col-lg-12'})\n",
    "    if tit== None:\n",
    "        tit = soup.find('h1', attrs={'class':'H2_1pmnvep-o_O-weightNormal_s9jwp5-o_O-fontHeadline_1uu0gyz max-text-width-xl m-b-1s'})\n",
    "        if tit==None :\n",
    "            tit = soup.find('h2', attrs={'class':'display-5-text degree-name'})\n",
    "            if tit== None:\n",
    "                title='Degrees & Certificates'\n",
    "            else:\n",
    "                title= tit.text\n",
    "        else:\n",
    "            title = tit.text\n",
    "    else:\n",
    "        title = tit.text\n",
    "    company = 'Empty()'\n",
    "    des = soup.find('div', attrs={'class':'AboutCourse'})\n",
    "    if des== None:\n",
    "        des=soup.find('div', attrs={'class':'rc-Column bt3-col-xs-12 bt3-col-md-7 rc-MainColumn'})\n",
    "        if des==None:\n",
    "            des=soup.find('div',attrs={'class':'content-inner'})\n",
    "            if des==None:\n",
    "                des='Degrees & Certificates'\n",
    "            else:\n",
    "                des=des.text\n",
    "        else:\n",
    "            des=des.find('div')\n",
    "            des=des.text\n",
    "    else :\n",
    "        des=des.text\n",
    "    \n",
    "    res=CourseDetailed(title, company, des)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in listkey:\n",
    "    # TÌM KIẾM KEYWORD TRONG LISTKEY\n",
    "    driver.get('(<TRANG WEB CẦN TÌM>')\n",
    "    # TÌM DIV CLASS CỦA Ô INPUT ĐỂ THAY ĐỔI\n",
    "    keyword = driver.find_element_by_xpath(\"//input[@id='q']\")\n",
    "    keyword.clear()\n",
    "    keyword.send_keys(i)\n",
    "    keyword.send_keys(Keys.RETURN)\n",
    "    #NẾU TRANG WEB K CHO TỰ NHẬP KEY THÌ TA LẤY URLS TÌM KIẾM NHẬP VÀO LISTKEY VÀ GET(i) \n",
    "    driver.get(i)\n",
    "    # KIỂM TRA NẾU TỪ KHÓA KHÔNG CÓ KẾT QUẢ NÀO THÌ BỎ QUA\n",
    "    xt= driver.find_elements_by_xpath(\"//div[@id='no_results']\")\n",
    "    if len(xt)>0:\n",
    "        continue\n",
    "    # BẮT ĐẦU LOAD TRANG\n",
    "    source_code = driver.page_source    \n",
    "    soup = bs(source_code, 'lxml')\n",
    "    while True:\n",
    "        SCROLL_PAUSE_TIME = 0.5\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "# Wait to load page-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "HIỆN TẠI CÓ 3 DẠNG CUỘN TRANG WEB CHÍNH LÀ\n",
    "----------------(1)-----------------------\n",
    "Cuộn để lấy thêm bài:\n",
    "- Data Science & Big Data Vietnam (fb)\n",
    "- Vietnam Data Scientists (fb)\n",
    "- Forum Machine Learning cơ bản (fb)\n",
    "----------------(2)-----------------------\n",
    "Nhấn xem thêm show thêm bài:\n",
    "- https://itviec.com\n",
    "- Internship.edu.vn\n",
    "----------------(3)-----------------------\n",
    "Sô thứ tự 1, 2, 3:\n",
    "- https://www.vietnamworks.com\n",
    "- https://www.glassdoor.com\n",
    "- https://www.careerbuilder.com\n",
    "- https://www.jobstreet.vn (edited) \n",
    "'''\n",
    "        for scr in range(1000):\n",
    "                driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "                time.sleep(1.5)\n",
    "                # TÌM KIẾM TẤT CẢ CÁC NÚT LOADMORE (DÀNH CHO ĐẠNG TRANG WEB Ở (2))\n",
    "                a=driver.find_elements_by_class_name('ais-InfiniteHits-loadMore')\n",
    "                if len(a)==0:\n",
    "                    #print(\"      no more job with this keyword, go to the next\")\n",
    "                    break\n",
    "                for cl in a:\n",
    "                    cl.click()\n",
    "# Wait to load page-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        #DẠNG CUỘN XUỐNG BOTTOM WEB SẼ TỰ LOAD THÊM DỮ LIỆU NHƯ (1) THÌ DÙNG :\n",
    "    '''\n",
    "     SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "    \n",
    "    '''\n",
    "# Wait to load page-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # DẠNG CLICK SỐ THỨ TỰ ĐỂ LOAD QUA TRANG KHÁC THÌ HƠI KHÁC 1 CHÚT\n",
    "    #VIỆC APPEND URL VÀ LOAD TRANG PHẢI THỰC HIỆN ĐỒNG THỜI VÀ XEN KẼ.\n",
    "    xt=driver.find_elements_by_link_text(\"Kế tiếp\")\n",
    "    if len(xt) == 0:\n",
    "        break\n",
    "    driver.find_element_by_link_text(\"Kế tiếp\").click()\n",
    "# Wait to load page-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#        PHẦN NÀY DÙNG ĐỂ TÌM KIẾM XÁC ĐỊNH CÓ POP-UP HAY KHÔNG -TÙY TRANG WEB\n",
    "        close=driver.find_elements_by_xpath(\"//div[@class='lean-overlay']\")\n",
    "        if(len(close))>0:\n",
    "            print(\"      !!![WARNING]:---have a fade in popup, wait a few second to close it before get the next page\")\n",
    "            time.sleep(1)\n",
    "            driver.find_element_by_xpath(\"//div[@id='job_alert_modal']//a\").click()\n",
    "            time.sleep(1)\n",
    "#            \n",
    "# all work will be here------------------------------------------------------------------------------------------\n",
    "        source_code = driver.page_source    \n",
    "        soup = bs(source_code, 'lxml')\n",
    "#        pre=soup.find('ul', attrs={'id':'jobresults'})\n",
    "        jobs=soup.find_all('li', attrs={'class':'ais-InfiniteHits-item'} )\n",
    "        print(len(jobs))\n",
    "        #       find element have link\n",
    "        for item in jobs:\n",
    "            base='https://coursera.org'\n",
    "            urlx=urljoin(base,item.select('a')[0]['href'] )\n",
    "            urls.append(urlx)\n",
    "#            print(urlx)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4770.39497923851\n"
     ]
    }
   ],
   "source": [
    "# Dùng future session và hàm Details để thu thập thông tin\n",
    "\n",
    "session = FuturesSession(max_workers=20)\n",
    "futures = [url for url in urls]\n",
    "    \n",
    "details = [detail(url) for url in urls]\n",
    "data_dict = {'url': urls, 'title': [detail.title for detail in details], 'company': [detail.company for detail in details], 'description':[detail.description for detail in details] }\n",
    "data_frame = pd.DataFrame(data_dict)\n",
    "end = time.time()\n",
    "data_frame.to_csv('COURSERA-ORG.csv')\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-9aaf1dd19b38>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-9aaf1dd19b38>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    nhìn lại khoảng thời gian cách đây 3 tuần: Huân đã học được khá nhiều thứ so với lúc không biết chút gì, phải bắt đầu, phải thử\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nhìn lại khoảng thời gian cách đây 3 tuần: Huân đã học được khá nhiều thứ so với lúc không biết chút gì, phải bắt đầu, phải thử\n",
    "    thì mới biết bản thân có thể hay không thể, nhưng nhìn vào task remaining (+ issues) và imagine về coming task thì thấy \n",
    "    bản thân em còn quá nhiều hạn chế, 2 tiếng mội ngày quả thật là không đủ cho việc tự học tự tìm hiểu và tự giải quyết vde\n",
    "    em thấy bản thân mình cần cố gắng nhiều thêm nữa. \n",
    "    đa phần dữ liệu cào về, các con số nhìn thấy được chỉ là số bài/keyword/web và thời gian từng trang.\n",
    "    nếu dùng để làm data visualize thì chắc chắn sẽ rất nghèo nàn và không biểu thị được nhiều thông tin.\n",
    "    và em cũng rất háo hức chờ tới phần xử lý dữ liệu, để từ dữ liệu thô trở nên có ích là như thế nào <3 em muốn viết thêm, nhưng bạn ngân k cho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
